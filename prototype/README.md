# ETL Prototyping Notebooks

## Overview

This folder contains Jupyter notebooks designed to prototype **ETL (Extract, Transform, Load)** flows for processing healthcare-related data. The workflows are intended to transform raw data files into cleaned, structured, and processed data for downstream analysis and reporting.

## Data Source

The raw data used in this project is downloaded from **[Synthea](https://synthea.mitre.org/downloads)**, a synthetic patient generator. Synthea produces high-quality, realistic, and entirely synthetic healthcare data that is **safe for use in research and development purposes**. No real patient data is used in this project, ensuring compliance with data privacy regulations.

## Data Flow

The ETL process involves three primary stages:

1. **Extract**:
   - Reading raw CSV files containing healthcare data (folder: data/raw):
     - `encounters.csv`
     - `patients.csv`
     - `organizations.csv`
     - `medications.csv`
     - `conditions.csv`
     - `procedures.csv`

2. **Transform**:
   - Cleaning and transforming the raw data to make it suitable for analysis:
     - Handling missing values.
     - Data type conversions.
     - Renaming columns for consistency.
     - Merging or splitting datasets based on relationships.
     - Applying business logic and creating derived columns (length_of_stay_hours, age etc.).

3. **Load**:
   - Saving the transformed data into new processed CSV files (save in folder: data/processed):
     - `processed_encounters.csv`
     - `processed_patients.csv`
     - `processed_organizations.csv`
     - `processed_medications.csv`
     - `processed_conditions.csv`
     - `processed_procedures.csv`

## Purpose

The primary objective of these notebooks is to:

- Prototype and validate ETL workflows.
- Ensure data quality and consistency across datasets.
- Generate structured data outputs that are ready for use in data analysis and visualization platforms such as PowerBI.

## Contents

- **Notebooks**:
  - Each notebook corresponds to a specific stage or dataset in the ETL process
    - rough_prototype_v1.ipynb: contains a quick prototypes of all functionalities that will be implemented for the downstream processes
  
- **Raw Data**:
  - Input CSV files representing raw, unprocessed data.

- **Processed Data**:
  - Output CSV files containing cleaned and transformed data.

## Usage

1. Clone this repository to your local machine.
2. Open the notebooks in Jupyter or your preferred IDE.
3. Place the raw CSV files in the appropriate directory as required by the notebooks.
4. Execute the notebooks sequentially to process the data.

## Next Steps

- These notebooks are **prototypes** and will serve as a foundation for building scalable and production-grade ETL pipelines.
- The final ETL pipelines should be automated and deployed using tools such as **Apache Airflow**, **Azure Data Factory**, or **Python scripts** for scheduled processing. If time permits, sample dag file will be provided for **Apache Airflow**

---

### Notes

- The data used in this project is entirely synthetic, generated by **Synthea**. It is safe and free to use for **research and development purposes**.
